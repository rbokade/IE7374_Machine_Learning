	(1) \textbf{True.} For any continuous random variable $x$ with a probability distribution function $p(x)$ the probability is bounded as $0 \leq p(x) \geq 1$ as $\int_{-\infty}^{\infty}p(x)dx = 1$ \\
	
	
	(2) \textbf{False}. If $X$ and $Y$ are independent, then $\mathbb{E}[XY^{2}] = \mathbb{E}[X]\mathbb{E}[Y^2]$ \\
	
	
	(3) \textbf{True}. True error is a hypothetical error value on the entire population. Hence, as $n \rightarrow \infty$ the training error will converge to the true error. \\
	
	
	(4) \textbf{False}. The linear regression is likely to have high bias and low variance whereas the polynomial regression with degree 3 is likely to have a low bias and high variance. \\
	
	
	(5) \textbf{False}. The empirical risk of $f_{1}$ will be greater than $f_{2}$. \\
	
	(6) \textbf{True}. Overfitting can occur due to a high approximation error as a result of overly simplistic hypothesis space or small quantity of data. \\
	
	
	(7) \textbf{False}. The approximation error will always be deterministic. \\
	
	
	(8) \textbf{True}. Linear regression has the lease variance. \\
	
	
	(9) \textbf{False}. The log likelihood of logistic regression is a convex function and will not have multiple local optima. \\
	
	
	(10) \textbf{False}. The correspondence implies that the posterior Gaussian Naive Bayes can be written in the same form as Logistic Regression.
