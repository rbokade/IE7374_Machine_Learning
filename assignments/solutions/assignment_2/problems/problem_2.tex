\begin{enumerate}
	
	
	\item Since, $\epsilon_{i} \sim \mathcal{N}(0, \sigma^{2})$, $y \sim \mathcal{N}(w^{T}x + b,\sigma^{2})$. Thus, we can write 
		\begin{align*}
    		p(y = y_{i} | x_{i}, w_{i}, b) & = \prod_{i=1}^{N} \frac{1}{\sqrt{2\pi\sigma}} \exp 			\left( -\frac{1}{2} \left( \frac{y_{i} - w_{i}x_{i} - b}{\sigma} \right)^{2} \right) 
		\end{align*}
		\href{https://stats.stackexchange.com/questions/327427/how-is-y-normally-distributed-in-linear-regression}{Reference}
		
	\item 
		\begin{align*}
			P(y | \beta) & = \prod_{i=1}^{N} p(y_{i} | x_{i}, w_{i}, b) \\
	    	& = \left( \frac{1}{\sqrt{2\pi\sigma}} \right)^{N} \exp -\frac{1}{2} \left( \frac{\sum_{i=1}^{n}(y_{i} - w_{i}x_{i} - b)^{2}}{\sigma^{2}} \right) \\
    		& \text{Taking log on both sides} \\
			\ln P(y | \beta) & = -n \log \sqrt{2\pi\sigma} - \frac{1}{2\sigma^{2}} \sum_{i=1}^{n} (y_{i} - w_{i}x_{i} - b)^{2}	
		\end{align*}
	
	
	
	\item 
		\begin{align*}
			ln P(y | \beta) & = -n \log \sqrt{2\pi\sigma} - \frac{1}{2\sigma^{2}} \sum_{i=1}^{n} (y_{i} - w_{i}x_{i} - b)^{2} \\
		\end{align*}
		Maximizing the log likelihood is equivalent to minimizing the "summation" in the second term
		\begin{align*}
			\max_{\beta} \ln P(y | \beta) & = \min_{\beta}(\sum_{i=1}^{n} (y_{i} - w_{i}x_{i} - b)^{2}) \\
    		& = arg\min_{\beta}(y - x^{T}\beta)^{T}(y - x^{T}\beta)
		\end{align*}
		Other terms are constant.
	
	
	
	\item To derive the values of coefficients $\beta$, we can take derivative of the log likelihood with respect to $\beta$ and equating it to zero.
		\begin{align*}
			J(\beta) & = (y - X\beta)^{T}((y - X\beta)) \\			
		\end{align*}
		Taking derivative with respect to $\beta$
		\begin{align*}
			\frac{\partial}{\partial\beta} J(\beta) & = 2X^{T}(X\beta - y) = 0 \\
		    & \therefore X^{T}(X\beta - y) = 0 \\
		    & \therefore X^{T}X\beta - X^{T}y = 0 \\
		    & \therefore \beta = (X^{T}X)^{-1}X^{T}y
		\end{align*}
		    		
		
\end{enumerate}